{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7914a57f-d250-44c9-a4ce-bfd4d43d4f96",
   "metadata": {},
   "source": [
    "**什么是协程？**\n",
    "\n",
    "协程是实现并发编程的一种方式。一说并发，你肯定想到了多线程/多进程模型，没错，多线程/多进程，正是解决并发问题的经典模型之一。最初的互联网世界，多线程/多进程在服务器并发中，起到了举足轻重的作用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effe57ab-5997-4e8b-b6c7-07a35ae4170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "OK url_1\n",
      "crawling url_2\n",
      "OK url_2\n",
      "crawling url_3\n",
      "OK url_3\n",
      "crawling url_4\n",
      "OK url_4\n",
      "CPU times: user 6.1 ms, sys: 4.92 ms, total: 11 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    time.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "def main(urls):\n",
    "    for url in urls:\n",
    "        crawl_page(url)\n",
    "\n",
    "%time main(['url_1', 'url_2', 'url_3', 'url_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d4df757-5a01-476f-bc6c-931631c4f57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "OK url_1\n",
      "crawling url_2\n",
      "OK url_2\n",
      "crawling url_3\n",
      "OK url_3\n",
      "crawling url_4\n",
      "OK url_4\n",
      "CPU times: user 7.79 ms, sys: 3.76 ms, total: 11.5 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "    \n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "async def main(urls):\n",
    "    for url in urls:\n",
    "        await crawl_page(url)\n",
    "\n",
    "%time asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218de201-595f-46fa-af9a-b6c831fb0957",
   "metadata": {},
   "source": [
    "10秒就对了，还记得上面所说的，await是同步调用，因此，crawl_page(url)在当前的调用结束之前，是不会触发下一次调用的。于是，这个代码效果和上面完全一样了，相当于我们用异步代码写了个同步代码。\n",
    "\n",
    "现在又该怎么办呢？\n",
    "\n",
    "其实很简单，也正是我接下来要讲的协程中的一个重要概念，任务（Task）。老规矩，先看代码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19345306-0ee3-45a0-9710-62b308e0b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await async.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
